{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand the intuition behind decision trees.\n",
    "- Calculate Gini.\n",
    "- Describe how decision trees use Gini to make decisions.\n",
    "- Fit, generate predictions from, and evaluate decision tree models.\n",
    "- Interpret and tune `criterion`, `max_depth`, `min_samples_split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we get for dinner?\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a rainy day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Indian food.\n",
    "- In 100% of past cases where the weather is rainy, we've eaten Indian food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a sunny day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Either Sushi or Mexican food... but we can't say with certainty whether we'd eat sushi or Mexican food.\n",
    "- Based on our past orders, we eat sushi on 1/3 of sunny days and we eat Mexican food on 2/3 of sunny days.\n",
    "- If I **had** to make a guess here, I'd probably predict Mexican food, but we may want to use additional information to be certain.\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a sunny day that also happens to be a weekend. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Mexican food.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've eaten Mexican food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that partitions (splits) our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "We frequently see decision trees represented by a graph.\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "Decision trees look like upside down trees. \n",
    "- What we see on top is known as the \"root node,\" through which all of our observations are passed.\n",
    "- At each internal split, our dataset is partitioned.\n",
    "- At each of the \"leaf nodes\" (colored orange), we contain a subset of records that are as pure as possible.\n",
    "    - In this food example, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify the idea of \"purity\" here... we'll come back to this later.\n",
    "\n",
    "Decision trees are also called \"**Classification and Regression Trees**,\" sometimes abbreviated \"**CART**.\"\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Questions\n",
    "\n",
    "If you aren't familiar with the game [20 Questions](https://en.wikipedia.org/wiki/Twenty_Questions), it's a game with two players (or teams). \n",
    "- Player A thinks of an item but doesn't say what the item is.\n",
    "- Player B then attempts to guess what the item is by asking a series of 20 questions with a yes or no answer.\n",
    "- If player B correctly guesses the item, then player B wins!\n",
    "- If player B does not correctly guess the item, then player A wins!\n",
    "\n",
    "Let's play a quick game of 20 Questions to get a feel for it.\n",
    "\n",
    "---\n",
    "\n",
    "Decision trees operate in a fashion that's pretty similar to 20 questions.\n",
    "- Decisions are made in a sequential fashion. Once you know a piece of information, you use that piece of information when asking future questions.\n",
    "    - Example: If you know that the item you're trying to guess is a person, then you can use that information to ask better subsequent questions.\n",
    "- It's possible to get lucky by making very specific guesses early, but it's pretty unlikely that this is a winning strategy.\n",
    "    - Example: If you asked, \"Is it an airplane? Is it a boat? Is it a car?\" as your first three questions, it's not very likely that you'll win the game.\n",
    "\n",
    "When fitting a decision tree, we're effectively getting our computer to play a game of 20 Questions. We give the computer some data and it figures out the best $X$ variable to split on at the right time.\n",
    "- Above, our \"what food should we order?\" decision tree first asked what the weather was, **then** asked whether it was a weekday or weekend.\n",
    "- If we had asked \"is it a weekday or weekend\" first, we'd have ended up with a slightly more complicated decision tree.\n",
    "\n",
    "Just like with all of our models, in order for the computer to learn which $X$ variable to split on and when, the computer needs a loss function to quantify how good a particular split is. This is where the idea of **purity** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "<img src=\"./images/order_food_dt.png\" alt=\"order_food\" width=\"750\"/>\n",
    "\n",
    "- For continuous $Y$ (i.e. using a decision tree to predict income), the default option is mean squared error.\n",
    "    - When the decision tree is figuring out which split to make at a given node, it picks the split that minimizes the MSE at that step.\n",
    "    \n",
    "    \n",
    "- For discrete $Y$, the default option is the Gini impurity. (This is not quite the same thing as the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).)\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- We use Gini to measure how pure a split is.\n",
    "- Gini impurity ranges from 0 to 0.5, where:\n",
    "    - 0 means \"most pure.\"\n",
    "    - 0.5 means \"least pure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['Indian', 'Sushi', 'Indian', 'Mexican', 'Indian', 'Mexican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \n",
    "    gini_sum = []\n",
    "    for class_i in set(y):\n",
    "        prob = (y.count(class_i) / len(y))\n",
    "        gini_sum.append(prob ** 2)\n",
    "\n",
    "    return 1 - sum(gini_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['Indian', 'Indian', 'Indian', 'Indian', 'Indian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican', 'Mexican', 'Mexican'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican', 'Indian', 'Mexican'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican', 'Sushi', 'Sushi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['bacon', 'bacon', 'bacon', 'broc', 'broc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21333333333333332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['bacon', 'bacon', 'bacon', 'broc', 'broc']) - (2/5 * gini(['bacon', 'bacon']) + 3/5 * gini(['broc', 'broc', 'bacon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17999999999999994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(['bacon', 'bacon', 'bacon', 'broc', 'broc']) - (1/5 * gini(['broc']) + 4/5 * gini(['bacon', 'bacon', 'bacon', 'broc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Under what circumstances should our Gini be 0?</summary>\n",
    "- When all of our observations are identical.\n",
    "</details>\n",
    "\n",
    "<details><summary>Under what circumstances should our Gini be 0.5?</summary>\n",
    "- When we have exactly two outcomes that are equally represented.\n",
    "</details>\n",
    "\n",
    "### So how does a decision tree use Gini to decide which variable to split on?\n",
    "\n",
    "- At any node, consider the subset of records that exist at that node.\n",
    "- Iterate through each variable that could potentially split the data.\n",
    "- Calculate the Gini impurity for every split.\n",
    "- Select the variable that causes the greatest decrease in Gini impurity from the parent node to the child node.\n",
    "\n",
    "One consequence of this is that a decision tree is fit using a **greedy** algorithm. Simply put, a decision tree makes the best short-term decision by optimizing at each node individually. This might mean that our tree isn't optimal in the long run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    ['DC', 'Batman', True, True, True],\n",
    "    ['DC', 'Superman', False, True, True],\n",
    "    ['DC', 'Flash', False, True, False],\n",
    "    ['DC', 'Wonder Woman', False, True, True],\n",
    "    ['DC', 'Aquaman', False, False, True],\n",
    "    ['Marvel', 'Iron Man', True, False, True],\n",
    "    ['Marvel', 'Captain America', False, False, False],\n",
    "    ['Marvel', 'Black Widow', True, False, False],\n",
    "    ['Marvel', 'Dare Devil', False, True, False],\n",
    "    ['Marvel', 'Spider-Man', False, True, True]\n",
    "], columns=['universe', 'name', 'tech', 'dead_parents', 'ends_with_man_or_woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "target = 'universe'\n",
    "\n",
    "parent = df[target].tolist()\n",
    "total = len(parent)\n",
    "\n",
    "print( total )\n",
    "print( gini(parent) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'tech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_1 = df[df[column] == True][target].tolist()\n",
    "len(child_1)\n",
    "gini(child_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48979591836734704"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_2 = df[df[column] != True][target].tolist()\n",
    "len(child_2)\n",
    "gini(child_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>universe</th>\n",
       "      <th>tech</th>\n",
       "      <th>dead_parents</th>\n",
       "      <th>ends_with_man_or_woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DC</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  universe  tech  dead_parents  ends_with_man_or_woman\n",
       "0       DC  True          True                    True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(column, target='universe'):\n",
    "    parent = df[target].tolist()\n",
    "    total = len(parent)\n",
    "    gini_parent = gini(parent)\n",
    "    \n",
    "    child_1 = df[df[column] == True][target].tolist()\n",
    "    total_child_1 = len(child_1)\n",
    "    gini_child_1 = gini(child_1)\n",
    "    \n",
    "    child_2 = df[df[column] != True][target].tolist()\n",
    "    total_child_2 = len(child_2)\n",
    "    gini_child_2 = gini(child_2)\n",
    "    \n",
    "    ig = gini_parent - (\n",
    "        gini_child_1 * total_child_1/total + \n",
    "        gini_child_2 * total_child_2/total\n",
    "    )\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech -> 0.023809523809523725\n",
      "dead_parents -> 0.08333333333333337\n",
      "ends_with_man_or_woman -> 0.08333333333333337\n"
     ]
    }
   ],
   "source": [
    "for column in ['tech', 'dead_parents', 'ends_with_man_or_woman']:\n",
    "    print(column, '->', calculate_gini(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in Titanic data.\n",
    "titanic = pd.read_csv('data/titanic_clean.csv')\n",
    "\n",
    "target = 'Survived'\n",
    "y = titanic[target]\n",
    "X = titanic.drop(target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                     Name   Sex   Age  SibSp  Parch  \\\n",
       "0            1       3  Braund, Mr. Owen Harris  male  22.0      1      0   \n",
       "\n",
       "   Fare Embarked  \n",
       "0  7.25        S  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    ('Pclass', None), \n",
    "    ('Sex', LabelEncoder()),\n",
    "    ('Age', None), \n",
    "    ('Fare', None),\n",
    "    ('Embarked', LabelEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = mapper.fit_transform(X_train)\n",
    "Z_test = mapper.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9919678714859438\n",
      "Score on testing set: 0.6915887850467289\n"
     ]
    }
   ],
   "source": [
    "print(f'Score on training set: {model.score(Z_train, y_train)}')\n",
    "print(f'Score on testing set: {model.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What conclusion would you make here?</summary>\n",
    "\n",
    "- Our model is **very** overfit to the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting a decision tree, your model will always grow until it nearly perfectly predicts every observation!\n",
    "- This is like playing a game of 20 questions, but instead calling it \"Infinite Questions.\" You're always going to be able to win!\n",
    "\n",
    "<details><summary>Intuitively, what might you try to do to solve this problem?</summary>\n",
    "    \n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Is there a way for us to stop our model from growing? (Yes!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Decision Trees\n",
    "There are two hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "    - In the 20 questions analogy, this is like how many questions we can ask?\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, minimum number of samples required to split is 2. That is, if there are two observations in a node, we can split it if maximum purity hasn't been reached!\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - \n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 5,\n",
    "                            min_samples_split = 7,\n",
    "                            min_samples_leaf = 3,\n",
    "                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=3, min_samples_split=7,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8674698795180723\n",
      "Score on testing set: 0.7757009345794392\n"
     ]
    }
   ],
   "source": [
    "print(f'Score on training set: {dt.score(Z_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's GridSearch to try to find the best tree.\n",
    "\n",
    "- Check [3, 5, 7, 10] for `max_depth`.\n",
    "- Check [5, 10, 15, 20] for `min_samples_split`.\n",
    "- Check [2, 3, 4, 5, 6, 7] for `min_samples_leaf`.\n",
    "- Run 5-fold cross-validation.\n",
    "\n",
    "<details><summary>How many models are being fit here?</summary>\n",
    "\n",
    "- 4 * 4 * 6 * 5 = 480 models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                    param_grid={'max_depth': [3, 5, 7, 10],\n",
    "                                'min_samples_split': [5, 10, 15, 20],\n",
    "                                'min_samples_leaf': [2, 3, 4, 5, 6, 7]},\n",
    "                    cv=5,\n",
    "                    verbose = 1,\n",
    "                    return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958678245544434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:    0.9s finished\n",
      "/Users/max/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "grid.fit(Z_train, y_train)\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=6, min_samples_split=15,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 15}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152610441767069"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8734939759036144\n",
      "Score on testing set: 0.7383177570093458\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 7,\n",
    "                            min_samples_split = 15,\n",
    "                            min_samples_leaf = 6,\n",
    "                            random_state = 42)\n",
    "\n",
    "dt.fit(Z_train, y_train)\n",
    "\n",
    "print(f'Score on training set: {dt.score(Z_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's so great about decision trees?\n",
    "\n",
    "\n",
    "### 1. We don't have to scale our data.\n",
    "Much in the same way that decision trees don't need much preprocessing in order to work, the base assumptions around how data is used and estimated, don't rely on scale to be effective.\n",
    "\n",
    "### 2. Decision trees don't care about how your data is distributed.\n",
    "\n",
    "Is your data heavily skewed or not normally distributed? Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed.\n",
    "\n",
    "### 3. Easy to interpret.\n",
    "\n",
    "The output of a decision tree is easy to interpet and thus relatable to non-technical people. (We'll talk about `feature_importance` later.)\n",
    "\n",
    "### 4. Speed.\n",
    "\n",
    "Decision trees are fit very quickly!\n",
    "\n",
    "> **Protip**\n",
    ">\n",
    "> Consider creating a benchmark using a decision tree to understand how one might behave on its own before using a more complicated method.  Do your best to understand how a simple model behaves on a set of data, before using a more complex model. This is a great practice to get into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the downsides of decision trees?\n",
    "\n",
    "\n",
    "### 1. Decision trees can very easily overfit.\n",
    "Decision trees often suffer from high error due to variance, so we need to take special care to avoid this. (Luckily, there are lots of algorithms designed to do exactly this!)\n",
    "\n",
    "### 2. Decision trees are locally optimal.\n",
    "Because we're making the best decision at each node (greedy), we might end up with a worse solution in the long run.\n",
    "\n",
    "### 3. Decision trees don't work well with unbalanced data.\n",
    "We often will bias our results toward the majority class. We need to take steps to avoid this as well! (Check out the `class_weight` parameter if you're interested.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {
    "0b28c6b3b13649718658d43e965c8062": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
