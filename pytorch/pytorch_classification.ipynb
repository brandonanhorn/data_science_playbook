{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a re-written example from the PyTorch docs. Data available [here](https://download.pytorch.org/tutorial/data.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('data/names/*.txt')\n",
    "\n",
    "backgrounds_and_names = {}\n",
    "for file_path in file_paths:\n",
    "    background = re.search('data\\/names\\/(.*?)\\.txt$', file_path).group(1)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        names = f.read()\n",
    "    backgrounds_and_names[background] = names.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in backgrounds_and_names.items()]))\n",
    "df = pd.melt(df, var_name='background', value_name='name')\n",
    "df = df.dropna()\n",
    "df = df[df['name'].apply(len) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['name']\n",
    "y = df['background']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = [list(n) for n in list(x_train.values)]\n",
    "flat_letters = flatten(split_names)\n",
    "train_letters = list(set(flat_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_encoder = LabelBinarizer()\n",
    "letter_encoder.fit(train_letters)\n",
    "\n",
    "def name_to_tensor(name):\n",
    "    split_name = list(name)\n",
    "    le_name = [letter_encoder.transform([c]) for c in split_name]\n",
    "    return torch.tensor(le_name, dtype=torch.float)\n",
    "\n",
    "name_to_tensor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_encoder = LabelEncoder()\n",
    "train_backgrounds = list(y_train.unique())\n",
    "background_encoder.fit(train_backgrounds)\n",
    "\n",
    "def background_to_tensor(background):\n",
    "    le_background = background_encoder.transform([background])\n",
    "    return torch.tensor(le_background, dtype=torch.int64)\n",
    "\n",
    "background_to_tensor(\"English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network\n",
    "\n",
    "We're going to make a network that looks like this:\n",
    "\n",
    "![](https://i.imgur.com/Z2xbySO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "n_letters = len(letter_encoder.classes_)\n",
    "n_backgrounds = len(background_encoder.classes_)\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_backgrounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8795, -2.9409, -2.8710, -2.9027, -2.7920, -2.8387, -2.9437, -2.8008,\n",
      "         -2.8144, -2.9405, -2.9254, -2.9094, -2.9415, -2.8964, -2.8892, -2.9217,\n",
      "         -2.8724, -2.9700]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-0.0452,  0.0563, -0.0370,  0.0177, -0.0790, -0.0104,  0.0990,  0.0704,\n",
      "         -0.0237, -0.0140, -0.0924, -0.0973, -0.0850, -0.0913, -0.0515,  0.0517,\n",
      "         -0.0401, -0.0275, -0.0749, -0.0981,  0.0357,  0.0369, -0.0060,  0.0179,\n",
      "          0.0641,  0.0205,  0.0101, -0.0422,  0.0695, -0.1249, -0.0058, -0.0011,\n",
      "          0.0419,  0.0044,  0.0373, -0.0453, -0.0554,  0.0755, -0.0608, -0.0332,\n",
      "         -0.0113,  0.0182,  0.0014,  0.1034,  0.0720, -0.0140,  0.0388,  0.0319,\n",
      "         -0.1307,  0.0935, -0.0881, -0.0020,  0.0872, -0.1200, -0.0342, -0.1050,\n",
      "         -0.0688, -0.0682, -0.0668,  0.0081, -0.0060,  0.0961, -0.0764,  0.0770,\n",
      "         -0.1114, -0.0030, -0.0268,  0.0025,  0.0138, -0.0496,  0.0044,  0.0429,\n",
      "          0.0016,  0.0304,  0.0519, -0.0020,  0.0802, -0.0306,  0.0457,  0.0642,\n",
      "          0.0270,  0.0902, -0.0372,  0.0219,  0.0193,  0.0617,  0.0002, -0.0509,\n",
      "         -0.0517, -0.0483,  0.1073, -0.0129,  0.0202, -0.0368,  0.0296,  0.0248,\n",
      "          0.0640,  0.0495,  0.1003,  0.0247,  0.0169, -0.0143,  0.0329,  0.0488,\n",
      "         -0.0993,  0.0805,  0.0291, -0.0662, -0.0142,  0.0004, -0.0114, -0.0145,\n",
      "          0.0418, -0.0014,  0.0440, -0.1073,  0.0042,  0.0520,  0.0741,  0.0182,\n",
      "          0.0086,  0.0072,  0.0346, -0.0272, -0.0398, -0.0830,  0.0054, -0.0440]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = name_to_tensor('James')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)\n",
    "print(next_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and\n",
    "\n",
    "   -  Keep hidden state for next letter\n",
    "\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(x, y):\n",
    "    x_tensor = name_to_tensor(x)\n",
    "    y_tensor = background_to_tensor(y)\n",
    "    \n",
    "    hidden = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(x_tensor.size()[0]):\n",
    "        output, hidden = rnn(x_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, y_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16059it [01:32, 173.83it/s]\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "for x, y in tqdm(zip(x_train, y_train)):\n",
    "    try: \n",
    "        output, loss = train(x, y)\n",
    "        current_loss += loss\n",
    "    except UnboundLocalError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def predict(x):\n",
    "    x_tensor = name_to_tensor(x)\n",
    "    \n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    for i in range(x_tensor.size()[0]):\n",
    "        output, hidden = rnn(x_tensor[i], hidden)\n",
    "        \n",
    "    y_hat_raw = output.detach().numpy()[0]\n",
    "    i = np.argmax(y_hat_raw)    \n",
    "    y_hat = background_encoder.inverse_transform([i])[0]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('James')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = []\n",
    "for x in x_test:\n",
    "    y_hat.append(predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame({\n",
    "    'x': x_test, \n",
    "    'y_true': y_test, \n",
    "    'y_hat': y_hat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6587795765877957"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Anhorn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
