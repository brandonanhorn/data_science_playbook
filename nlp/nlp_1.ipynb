{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='common'></a>\n",
    "## Common NLP Problems\n",
    "\n",
    "---\n",
    "The applications of using text data in statistical modeling are practically infinite. Some examples include:\n",
    "\n",
    "**Sentiment Analysis** | Determining if what is written is positive or negative (e.g. movie reviews, restaurant reviews, tweets) or if a political writer is left-leaning or right-leaning. \n",
    "\n",
    "**Named Entity Recognition** | Classifying names of people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "**Summarization** | Boiling down large bodies of text to paraphrased versions.\n",
    "\n",
    "**Topic Modeling** | Pinpointing the topics a body of text belongs to (e.g., auto-tagging news articles). |\n",
    "\n",
    "**Question Answering** | Determining the answer to a human-language question.\n",
    "\n",
    "**Word Disambiguation** | Many words have more than one meaning; we have to select the meaning that makes the most sense in context. For this problem, we’re typically given a list of words and associated word senses (e.g., from a dictionary or from an online resource such as WordNet).\n",
    "\n",
    "**Machine Dialog Systems** | Building response systems that react contextually to human input (i.e., Me: \"Siri, cook me some bacon.\" Siri: \"How do you like your bacon cooked?\"). \n",
    "\n",
    "**Language Detection** | Determining what natural language a given text is written in, deciphering between similar languages and dialects (e.g. Serbian vs Croatian, Indonesian vs Malay, Québécois vs European French).\n",
    "\n",
    "**Machine Translation** | Converting from one natural language to another while preserving the meaning and producing valid output.\n",
    "\n",
    "**Pragmatic Analysis** | Going from what is _said_ to what is _meant_, which required context awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to Text Feature Extraction\n",
    "\n",
    "---\n",
    "\n",
    "The models we’ve been using so far accept a two-dimensional matrix of real numbers as input `X` and a target vector of classes or numbers as `y`. \n",
    "\n",
    "What if our starting data is unstructured, qualitative data (e.g. documents of text) instead of structure and quantitative?\n",
    "\n",
    "We need a way to fit unstructured data into our usual numeric `X` matrix in order to use the same models. This process is called _feature extraction_.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='simple'></a>\n",
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer, chairman of the board of directors of PJSC \"LUKOIL.\" I am 86-years old and I was diagnosed with cancer two years ago. I will be going in for an operation later this week. I decided to will/donate the sum of 8,750,000.00 Euros (eight million seven hundred and fifty thousand euros only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of data scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews, and we would like to invite you for an onsite interview with our senior data scientist, Mr. John Smith. You will find attached to this message further information on date, time, and location of the interview. Please let me know if I can be of any further assistance. Best regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer, chairman of the board of directors of PJSC \"LUKOIL.\" I am 86-years old and I was diagnosed with cancer two years ago. I will be going in for an operation later this week. I decided to will/donate the sum of 8,750,000.00 Euros (eight million seven hundred and fifty thousand euros only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of data scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews, and we would like to invite you for an onsite interview with our senior data scientist, Mr. John Smith. You will find attached to this message further information on date, time, and location of the interview. Please let me know if I can be of any further assistance. Best regards.\n",
    "\"\"\"\n",
    "\n",
    "print(spam)\n",
    "print()\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some NLP Terminology\n",
    "\n",
    "---\n",
    "\n",
    "- a collection of text is a **document**\n",
    "- a collection of documents is a **corpus** (plural corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [spam, ham]  # two documents in our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can You Think of a Simple Heuristic Rule to Catch Emails Like This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> _We could check for the presence of words such as \"donate\", \"will\", \"sum\", \"cancer\", \"LinkedIn\"_\n",
    "\n",
    "By defining a simple rule that parses the text for the presence of keywords, we’re performing one of the simplest text feature extraction methods: _word counting_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bow'></a>\n",
    "## Bag of Words/Word Counting\n",
    "---\n",
    "<img src=\"https://c1.staticflickr.com/5/4070/4654257115_aab01d4e37_b.jpg\" width=\"300px\"/>\n",
    "\n",
    "The bag-of-words model is a simplified representation of the raw data. In this model, a document is represented as the bag of its words.\n",
    "\n",
    "Bag-of-words representations discard grammar, order, and structure in the text and only tracks occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Now we can have numeric X features again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"countvectorizer\"></a>\n",
    "## Demo: Scikit-Learn `CountVectorizer`\n",
    "---\n",
    "\n",
    "Scikit-learn offers a `CountVectorizer` class:\n",
    "> \"Convert a collection of text documents to a matrix of token counts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x68 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 71 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [spam, ham]  # two documents in our corpus\n",
    "\n",
    "cvec.fit(corpus) # count vectorizer learns the vocabulary of the corpus\n",
    "matrix_corpus = cvec.transform(corpus)\n",
    "matrix_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 2,\n",
       "         1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_corpus.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>contact</th>\n",
       "      <th>euros</th>\n",
       "      <th>000</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>mr</th>\n",
       "      <th>old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      years  contact  euros  000  linkedin  lukoil  major  million  mr  old\n",
       "spam      2        2      2    1         1       1      1        1   1    1\n",
       "ham       0        0      0    0         0       0      0        0   1    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(matrix_corpus.todense(),\n",
    "                   columns=cvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "\n",
    "df.T.sort_values('spam', ascending=False).head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 68)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "## Stop Words\n",
    "\n",
    "---\n",
    "\n",
    "Some words are commonly used and provide no legitimate information about the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'among', 'up', 'get', 'the', 'because', 'thus', 'put', 'your', 'system', 'someone', 'other', 'five', 'in', 'becoming', 'thence', 'sincere', 'would', 'into', 'much', 'namely', 'move', 'via', 'whereby', 'whither', 'once', 'besides', 'hasnt', 'done', 'ie', 'myself', 'she', 'this', 'fifteen', 'may', 'over', 'against', 'without', 'before', 'serious', 'nine', 'found', 'fifty', 'un', 'no', 'least', 'as', 'mostly', 'until', 'each', 'whatever', 'from', 'name', 'thereupon', 'everywhere', 'onto', 'two', 'their', 'yet', 'is', 'top', 'empty', 'otherwise', 'everything', 'further', 'whenever', 'beyond', 'after', 'seeming', 'bottom', 'front', 'last', 'below', 'was', 'see', 'back', 'might', 'some', 'what', 'again', 'show', 'toward', 'both', 'ten', 'with', 'us', 'thru', 'moreover', 'ever', 'yours', 'be', 'will', 'latterly', 'latter', 'made', 'nowhere', 'could', 'amongst', 'indeed', 'while', 'anyway', 'whereas', 'keep', 'due', 'part', 'than', 'or', 'which', 'even', 'third', 'often', 'his', 'them', 'twenty', 'between', 'former', 'towards', 'nobody', 'has', 'several', 'whether', 'about', 'upon', 'must', 'almost', 'on', 'therefore', 'perhaps', 'who', 'becomes', 'all', 'my', 'were', 'only', 'also', 'itself', 'here', 'herself', 'hundred', 'out', 'thin', 'any', 'four', 'but', 'wherein', 'nevertheless', 'many', 'though', 'when', 'where', 'others', 'around', 'elsewhere', 'however', 'eg', 'to', 'thereafter', 'couldnt', 'anywhere', 'that', 'mill', 'three', 'too', 'become', 'herein', 'fill', 'if', 'himself', 'go', 'along', 'never', 'above', 'eleven', 'bill', 'seems', 'one', 'yourself', 'how', 'de', 'own', 'give', 'ours', 'afterwards', 'con', 'wherever', 'most', 'and', 'less', 'me', 'per', 'etc', 'mine', 'sixty', 'find', 'should', 'fire', 'beside', 'whoever', 'he', 'became', 'being', 'we', 'not', 'amoungst', 'well', 'another', 'side', 'call', 'whence', 'none', 'ltd', 'more', 'her', 'already', 'an', 'amount', 'except', 'seemed', 'through', 'inc', 'such', 're', 'twelve', 'do', 'i', 'same', 'been', 'everyone', 'hers', 'else', 'enough', 'therein', 'somehow', 'they', 'thereby', 'can', 'beforehand', 'at', 'please', 'formerly', 'detail', 'although', 'anyhow', 'every', 'interest', 'there', 'seem', 'thick', 'forty', 'throughout', 'behind', 'its', 'whereupon', 'whose', 'across', 'sometime', 'rather', 'eight', 'either', 'anyone', 'whom', 'neither', 'off', 'him', 'still', 'since', 'a', 'hence', 'of', 'themselves', 'so', 'hereafter', 'full', 'for', 'yourselves', 'next', 'always', 'hereupon', 'describe', 'am', 'together', 'cry', 'then', 'down', 'co', 'within', 'alone', 'nor', 'nothing', 'had', 'cannot', 'cant', 'by', 'very', 'during', 'it', 'now', 'six', 'whole', 'why', 'have', 'sometimes', 'first', 'noone', 'these', 'something', 'under', 'few', 'somewhere', 'ourselves', 'meanwhile', 'take', 'hereby', 'those', 'whereafter', 'you', 'our', 'anything', 'are'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    " \n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at the [count vectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and see what some of its parameters do.\n",
    "\n",
    "In particular, look at what the following do:\n",
    "\n",
    "- `encoding`\n",
    "- `analyzer`\n",
    "- `lowercase`\n",
    "- `stop_words`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and try eliminating stop words from our CountVectorizer's feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "- The unit varies: character, word, etc (in `CountVectorizer` default is word, but you can change this with `analyzer` param)\n",
    "\n",
    "If we're talking about words:\n",
    "\n",
    "- uni-gram (1-gram) is 1 word\n",
    "- bi-gram (2-gram) is 2 adjacent words\n",
    "- tri-gram (3-gram) is 3 adjacent words\n",
    "\n",
    "The sentence `I am counting words` contains:\n",
    "- four uni-grams: \n",
    "    * `I`\n",
    "    * `am`\n",
    "    * `counting`\n",
    "    * `words`  \n",
    "- three bi-grams: \n",
    "    * `I am` \n",
    "    * `am counting`\n",
    "    * `counting words`   \n",
    "    \n",
    "- two tri-grams: \n",
    "    * `I am counting`\n",
    "    * `am counting words` \n",
    "    \n",
    "    \n",
    "If we're talking about characters instead of words:\n",
    "\n",
    "- uni-gram (1-gram) is 1 character\n",
    "- bi-gram (2-gram) is 2 adjacent characters\n",
    "- tri-gram (3-gram) is 3 adjacent characters\n",
    "\n",
    "e.g. `_I`, `I_`, `_a`, `am`, `m_`, `_c`, `co`, `ou`, `un`, `nt`, `ti`, `in`, `ng`, `g_` are the bi-grams contained in `I am counting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"hashingvectorizer\"></a>\n",
    "## Scikit-Learn's `HashingVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "Although we can set the `CountVectorizer` dictionary to a fixed size (using the `max_features` param), only keeping words with the highest frequencies. However, **we still have to compute a dictionary and hold it in memory.** This could be a problem when...\n",
    "\n",
    "- we have a large corpus or \n",
    "- when we stream applications where we don't know which words we'll encounter in the future\n",
    "\n",
    "Both problems can be solved using the `HashingVectorizer`, which works similarly to `CountVctorizer`, but uses the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each word is mapped to a feature with the use of a [hash function](https://en.wikipedia.org/wiki/Hash_function), which converts it from a word to a \"hash\". If we encounter that word again in the text, it will be converted to the _same_ hash, allowing us to count word occurrence without retaining a dictionary in memory.\n",
    "\n",
    "<img src=\"images/hashing.png\" width=\"500px\" />\n",
    "\n",
    "$$\n",
    "\\text{word} \\rightarrow \\text{hash} \\rightarrow \\text{column index}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hash'></a>\n",
    "## What is a Hash Function?\n",
    "\n",
    "---\n",
    "<div style=\"overflow-y: hidden; height: 250px\">\n",
    "<img src=\"https://i.ytimg.com/vi/bs7Wq0Z1uYk/maxresdefault.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing\n",
    "\n",
    "A hash value is a number generated from a string of text. It's also referred to simply as \"hash\" or \"message digest.\"\n",
    "\n",
    "The hash is substantially smaller than the text itself and is generated by a formula in such a way that it's extremely unlikely some other text will produce the same hash value.\n",
    "\n",
    "Think of the hash as a code that represents the original text in a more condensed format.\n",
    "\n",
    "![](images/hash_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main drawback\n",
    "\n",
    "The hashing trick is *not reversible* - we can't tell which original words correspond with the important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's repeat the vectorization using a `HashingVectorizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>288820</th>\n",
       "      <th>449291</th>\n",
       "      <th>484920</th>\n",
       "      <th>816269</th>\n",
       "      <th>543187</th>\n",
       "      <th>826540</th>\n",
       "      <th>51251</th>\n",
       "      <th>608416</th>\n",
       "      <th>752544</th>\n",
       "      <th>946487</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      288820  449291  484920  816269  543187  826540  51251   608416  752544  \\\n",
       "spam     2.0     2.0     2.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "ham      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      946487  \n",
       "spam     1.0  \n",
       "ham      0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hvec = HashingVectorizer(stop_words='english', norm=None, alternate_sign=False)\n",
    "hvec.fit(corpus)\n",
    "matrix_corpus = hvec.transform(corpus)\n",
    "\n",
    "df  = pd.DataFrame(matrix_corpus.todense(), index=['spam', 'ham'])  \n",
    "\n",
    "df.T.sort_values('spam', ascending=False).head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1048576)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What new parameters does this vectorizer offer?\n",
    "\n",
    "Check out [the `HashingVectorizer` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) and compare it to [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"downsides-bow\"></a>\n",
    "## Downsides to Bag of Words\n",
    "\n",
    "---\n",
    "\n",
    "Bag-of-word approaches ignore the structure of a sentence and merely assess the presence of specific words or word combinations.\n",
    "\n",
    "\n",
    "When counting only unigrams, these two documents produce identical bags of words:\n",
    "\n",
    "\n",
    "- It was bad. It was not good.\n",
    "- It was good. It was not bad. <img src=\"images/bag_of_words.png\" width=\"180px\"/>\n",
    "\n",
    "\n",
    "The same word can have multiple meanings in different contexts:\n",
    "\n",
    "- There's wood floating in the **sea**.\n",
    "- Mike's in a **sea** of trouble with the move.\n",
    "\n",
    "\n",
    "How do we teach a computer to disambiguate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"tfidf\"></a>\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "---\n",
    "\n",
    "A TF-IDF score tells us which words are most discriminating between documents. Words that occur often in one document but don't occur in many documents contain a great deal of discriminating power.\n",
    "\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection (corpus).\n",
    "- The importance increases in proportion to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Variations of the TF-IDF weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides — that is, whether the term is common or rare across all documents. \n",
    "\n",
    "The idf of a rare term is high, whereas the idf of a frequent term is likely to be low.\n",
    "\n",
    "\n",
    "**Let's see how it's calculated:**\n",
    "\n",
    "Term frequency (`tf`) is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{term}$ is the number of times a term/word $t$ appears in document $d$\n",
    "- $N_\\text{terms in Document}$ is the number of terms/words in document $d$\n",
    "\n",
    "Inverse document frequency (`idf`) of a term is calculated by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{Documents}$ is the number of documents in the corpus $D$\n",
    "- $N_\\text{Documents that contain term}$ is the number of documents in $D$ that contain term/word $t$\n",
    "\n",
    "TF-IDF is then calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **You might ask: But what is `log` used for?**<br>\n",
    "> Good question!\n",
    "\n",
    "The log function looks like this:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Graph_of_common_logarithm.svg/250px-Graph_of_common_logarithm.svg.png)\n",
    "\n",
    "Taking the log helps \"dampen\" the effect of very high values.  \n",
    "\n",
    "For example, a document containing a term 2 million times is not twice as relevant as a document containing the same term 1 million times.  They are both very relevant documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='tfidf-vec'></a>\n",
    "## Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use TF-IDF?\n",
    "- Common words are penalized.\n",
    "- Rare words have more influence.\n",
    "\n",
    "Scikit-learn provides a TF-IDF vectorizer that works similarly to the other vectorizers we've covered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>contact</th>\n",
       "      <th>personality</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>old</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         years     euros   contact  personality  linkedin    lukoil     major  \\\n",
       "spam  0.290133  0.290133  0.290133     0.145067  0.145067  0.145067  0.145067   \n",
       "ham   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       million       old  operation  \n",
       "spam  0.145067  0.145067   0.145067  \n",
       "ham   0.000000  0.000000   0.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_matrix = tvec.transform(corpus)\n",
    "\n",
    "df  = pd.DataFrame(corpus_matrix.todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "\n",
    "df.transpose().sort_values('spam', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scientist</th>\n",
       "      <th>regards</th>\n",
       "      <th>data</th>\n",
       "      <th>interview</th>\n",
       "      <th>round</th>\n",
       "      <th>hooli</th>\n",
       "      <th>inform</th>\n",
       "      <th>interviews</th>\n",
       "      <th>invite</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scientist  regards     data  interview     round     hooli    inform  \\\n",
       "spam    0.00000  0.00000  0.00000    0.00000  0.000000  0.000000  0.000000   \n",
       "ham     0.31039  0.31039  0.31039    0.31039  0.155195  0.155195  0.155195   \n",
       "\n",
       "      interviews    invite      like  \n",
       "spam    0.000000  0.000000  0.000000  \n",
       "ham     0.155195  0.155195  0.155195  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transpose().sort_values('ham', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Real\" Example\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this stuff out on some SMS text data.  Can you predict real vs. promotional texts just based on what is written?  Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/sms.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865985\n",
       "spam    0.134015\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df['text'].values\n",
    "y = df['class'].map({'ham':0, 'spam':1})\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "x_train_counts = cvec.fit_transform(x_train)\n",
    "x_test_counts = cvec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or:\n",
    "# tfidf_vec = TfidfVectorizer()\n",
    "# x_train_counts = tfidf_vec.fit_transform(x_train)\n",
    "# x_test_counts = tfidf_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9820652173913044"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train_counts, y_train)\n",
    "log_reg.score(x_test_counts, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg.predict(x_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(predictions, y_test, x_test)), columns=['predictions', 'label', 'text_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>label</th>\n",
       "      <th>text_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Kit Strip - you have been billed 150p. Netcoll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Call Germany for only 1 pence per minute! Call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I'd like to tell you my deepest darkest fantas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Urgent Ur £500 guaranteed award is still uncla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Loans for any purpose even if you have Bad Cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CALL 09090900040 &amp; LISTEN TO EXTREME DIRTY LIV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>tddnewsletter@emc1.co.uk (More games from TheD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello darling how are you today? I would love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fantasy Football is back on your TV. Go to Sky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Check Out Choose Your Babe Videos @ sms.shsex....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A link to your picture has been sent. You can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you realize that in about 40 years, we'll h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Burger King - Wanna play footy at a top stadiu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reminder: You have not downloaded the content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You have 1 new message. Please call 08712400200.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Missed call alert. These numbers called but le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Please call Amanda with regard to renewing or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM 88066 LOST £12 HELP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Call FREEPHONE 0800 542 0578 now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You have 1 new message. Please call 08715205273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Email AlertFrom: Jeri StewartSize: 2KBSubject:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100 dating service cal;l 09064012103 box334sk38ch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sorry I missed your call let's talk when you h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for 078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Did you hear about the new \"Divorce Barbie\"? I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Are you unique enough? Find out from 30th Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You have an important customer service announc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fantasy Football is back on your TV. Go to Sky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The current leading bid is 151. To pause this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1786</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello. We need some posh birds and chaps to us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Voucher Holder 2 claim your 1st class air...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predictions  label                                           text_msg\n",
       "26              0      1  Kit Strip - you have been billed 150p. Netcoll...\n",
       "30              0      1  Call Germany for only 1 pence per minute! Call...\n",
       "45              0      1  I'd like to tell you my deepest darkest fantas...\n",
       "295             0      1  Urgent Ur £500 guaranteed award is still uncla...\n",
       "300             0      1                                      2/2 146tf150p\n",
       "320             0      1  Loans for any purpose even if you have Bad Cre...\n",
       "366             0      1  CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "459             0      1  tddnewsletter@emc1.co.uk (More games from TheD...\n",
       "524             0      1  Hello darling how are you today? I would love ...\n",
       "533             0      1  Fantasy Football is back on your TV. Go to Sky...\n",
       "562             0      1  Check Out Choose Your Babe Videos @ sms.shsex....\n",
       "609             0      1  A link to your picture has been sent. You can ...\n",
       "612             0      1  Do you realize that in about 40 years, we'll h...\n",
       "626             0      1  Burger King - Wanna play footy at a top stadiu...\n",
       "719             0      1  Reminder: You have not downloaded the content ...\n",
       "780             0      1  Oh my god! I've found your number again! I'm s...\n",
       "794             0      1   You have 1 new message. Please call 08712400200.\n",
       "836             0      1  Missed call alert. These numbers called but le...\n",
       "858             0      1  Please call Amanda with regard to renewing or ...\n",
       "870             0      1                           FROM 88066 LOST £12 HELP\n",
       "888             0      1                  Call FREEPHONE 0800 542 0578 now!\n",
       "1012            0      1    You have 1 new message. Please call 08715205273\n",
       "1084            0      1  Email AlertFrom: Jeri StewartSize: 2KBSubject:...\n",
       "1168            0      1  100 dating service cal;l 09064012103 box334sk38ch\n",
       "1192            0      1  Sorry I missed your call let's talk when you h...\n",
       "1234            0      1       PRIVATE! Your 2003 Account Statement for 078\n",
       "1305            0      1  Did you hear about the new \"Divorce Barbie\"? I...\n",
       "1512            0      1  Are you unique enough? Find out from 30th Augu...\n",
       "1570            0      1  You have an important customer service announc...\n",
       "1574            0      1  Fantasy Football is back on your TV. Go to Sky...\n",
       "1757            0      1  The current leading bid is 151. To pause this ...\n",
       "1786            0      1  Hello. We need some posh birds and chaps to us...\n",
       "1795            0      1  Dear Voucher Holder 2 claim your 1st class air..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.predictions != df.label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- Documentation: \n",
    "    - [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "    - [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). \n",
    "    - [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "- A list of stop words is available [here](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-05/5.04-nlp/stop-words.txt).\n",
    "- Wikipedia's [feature hashing](https://github.com/generalassembly-studio/DSI-course-materials/tree/master/curriculum/04-lessons/week-06/4.1-lesson) and [hash functions](https://en.wikipedia.org/wiki/Hash_function) entries are a great place to turn for more information on hashing.\n",
    "- [Feature hashing](https://en.wikipedia.org/wiki/Feature_hashing).\n",
    "- [Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "- Check out Charlie Greenbacker's [introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf), which he delivered at the [DC-NLP Meetup](http://www.meetup.com/DC-NLP/).\n",
    "- Wikipedia also has a [walk through](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) of TF-IDF.\n",
    "- Google's [ngram tool](https://books.google.com/ngrams/graph?content=data+science&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cdata%20science%3B%2Cc0).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
