{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gridsearching Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/aYcCt2.jpg)\n",
    "\n",
    "### Learning Objective\n",
    "- Describe what the terms gridsearch and hyperparameter mean.\n",
    "- Build a gridsearching procedure from scratch.\n",
    "- Apply sklearn's `GridSearchCV` object with basketball data to optimize a KNN model.\n",
    "- Use and evaluate attributes of the gridsearch object.\n",
    "- Describe the pitfalls of searching large hyperparameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is \"grid searching\"? What are \"hyperparameters\"?](#intro)\n",
    "- [Basketball Data](#basketball-data)\n",
    "- [Fitting a Default KNN](#fit-knn)\n",
    "- [Searching for the Best Hyperparameters](#searching)\n",
    "    - [Grid Search Pseudocode](#pseudocode)\n",
    "    - [Using `GridSearchCV`](#gscv)\n",
    "- [A Caution on Grid Searching](#caution)\n",
    "- [Independent Practice: Grid Searching Regularization Penalties with Logistic Regression](#practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## What is \"Grid Searching\"? What are \"Hyperparameters\"?\n",
    "\n",
    "---\n",
    "\n",
    "Models often have built-in specifications that we can use to fine-tune our results. For example, when we choose a linear regression, we may decide to add a penalty to the loss function such as the Ridge or the Lasso. Those penalties require the regularization strength, alpha, to be set. \n",
    "\n",
    "**These specifications are called hyperparameters.**\n",
    "\n",
    "Hyperparameters are different from the parameters of the model that result from a fit, such as the coefficients. They are set prior to the fit - usually when we instantiate it - and they affect or determine the model's behavior.\n",
    "\n",
    "There are often more than one kind of hyperparamter to set for a model. For example, in the KNN algorithm, we have a hyperparameter to set the number of neighbors. We also have a hyperparameter to set the weights, eithe uniform or distance. Generally, we want to know the *optimal* hyperparameter settings, the set that results in the best model evaluation. \n",
    "\n",
    "**The search for the optimal set of hyperparameters is called gridsearching.**\n",
    "\n",
    "Gridsearching gets its name from the fact that we are searching over a \"grid\" of parameters. For example, imagine the `n_neighbors` hyperparameters on the x-axis and `weights` on the y-axis, and we need to test all points on the grid.\n",
    "\n",
    "**Gridsearching uses cross-validation internally to evaluate the performance of each set of hyperparameters.** More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basketball-data'></a>\n",
    "\n",
    "## Basketball Data\n",
    "\n",
    "---\n",
    "\n",
    "To explore the process of gridsearching over sets of hyperparameters, we will use some basketball data. The data below has statistics for 4 different seasons of NBA basketball: 2013-2016.\n",
    "- This data includes aggregate statistical data for each game. \n",
    "- The data of each game is aggregated by match for all players.\n",
    "- Scraped from http://www.basketball-reference.com\n",
    "\n",
    "Many of the columns in the dataset represent the mean of a statistic across the last 10 games, for example. Non-target statistics are for *prior* games, they do not include information about player performance in the current game.\n",
    "\n",
    "**We are interested in predicting whether the home team will win the game or not.** This is a classification problem.\n",
    "\n",
    "\n",
    "### Load the data and create the target and predictor matrix\n",
    "- The target will be a binary column of whether the home team wins.\n",
    "- The predictors should be numeric statistics columns.\n",
    "\n",
    "Exclude these columns from the predictor matrix:\n",
    "\n",
    "    ['GameId','GameDate','GameTime','HostName',\n",
    "     'GuestName','total_score','total_line','game_line',\n",
    "     'winner','loser','host_wins','Season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/basketball_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'GameId', 'GameDate', 'GameTime', 'HostName', 'GuestName',\n",
       "       'total_score', 'total_line', 'game_line', 'Host_HostRank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "predictors = list(\n",
    "    set(data.columns) - set(['GameId','GameDate','GameTime','HostName',\n",
    "                              'GuestName','total_score','total_line','game_line',\n",
    "                              'winner','loser','host_wins','Season'])\n",
    ")\n",
    "\n",
    "df = data[predictors].copy()\n",
    "X = df\n",
    "\n",
    "# Create a binary int column to represent host's win/loss\n",
    "y = (data.HostName == data.winner).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "mapper = DataFrameMapper([], default=StandardScaler())\n",
    "\n",
    "Z_train = mapper.fit_transform(X_train)\n",
    "Z_test = mapper.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-knn'></a>\n",
    "\n",
    "## Fitting the Default KNN\n",
    "\n",
    "---\n",
    "\n",
    "Below we can fit a default `KNeighborsClassifier` to predict win vs. not on the training data, then score it on the testing data. \n",
    "\n",
    "Remember to compare your score to the baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = KNeighborsClassifier(n_neighbors=11)\n",
    "neighbors.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7038216560509554"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors.score(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.613588110403397"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors.score(Z_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5987261146496815\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='searching'></a>\n",
    "\n",
    "## Searching for the Best Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "Our default KNN performs quite poorly on the test data. But what if we changed the number of neighbors? The weighting? The distance metric?\n",
    "\n",
    "These are all hyperparameters of the KNN algorithm. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set.\n",
    "\n",
    "<a id='pseudocode'></a>\n",
    "### Gridsearch pseudocode for our KNN\n",
    "\n",
    "```python\n",
    "accuracies = {}\n",
    "for k in neighbors_to_test:\n",
    "    for w in weightings_to_test:\n",
    "        for d in distance_metrics_to_test:\n",
    "            hyperparam_set = (k, w, d)\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, metric=d)\n",
    "            cv_accuracies = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "            accuracies[hyperparam_set] = np.mean(cv_accuracies)\n",
    "```\n",
    "\n",
    "In the pseudocode above, we would find the key in the dictionary (a hyperparameter set) that has the largest value (mean cross-validated accuracy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gscv'></a>\n",
    "### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily sklearn comes with a convenience class for performing gridsearch:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the gridsearch for our KNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[1,3,5,7,9],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the sklearn gridsearch below on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'metric': ['euclidean', 'manhattan'],\n",
       "                         'n_neighbors': [1, 3, 5, 9, 15, 21],\n",
       "                         'weights': ['uniform', 'distance']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':[1,3,5,9,15,21],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "knn_gridsearch.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gs-results'></a>\n",
    "### Examining the results of the gridsearch\n",
    "\n",
    "Once the gridsearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the gridsearch object, stored as attributes:\n",
    "\n",
    "| Property | Use |\n",
    "| --- | ---|\n",
    "| **`results.param_distributions`** | Displays parameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The parameters that have been found to perform with the best score. |\n",
    "| **`results.cv_results`** | Display score attributes with corresponding parameters. (make sure to use return_train_score=True with GridSearch) | \n",
    "\n",
    "**Print out the best score found in the search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'metric': ['euclidean', 'manhattan'],\n",
       "                         'n_neighbors': [1, 3, 5, 9, 15, 21],\n",
       "                         'weights': ['uniform', 'distance']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining `grid.cv_results_`\n",
    "\n",
    "The docs have a full description of the results, and you can check them out in the [sklearn docs for GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).  It's likely not a coincidence that the `grid.cv_results_` are in a convienent format that makes them easy to import into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6199575371549894"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_metric</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>param_weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.398121</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 1, 'wei...</td>\n",
       "      <td>0.549470</td>\n",
       "      <td>0.552212</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.568142</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.563694</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.374393</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 1, 'wei...</td>\n",
       "      <td>0.549470</td>\n",
       "      <td>0.552212</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.568142</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.563694</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016816</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.388486</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 3, 'wei...</td>\n",
       "      <td>0.574205</td>\n",
       "      <td>0.546903</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>0.576991</td>\n",
       "      <td>0.605310</td>\n",
       "      <td>0.582095</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016576</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.369897</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 3, 'wei...</td>\n",
       "      <td>0.574205</td>\n",
       "      <td>0.546903</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>0.576991</td>\n",
       "      <td>0.605310</td>\n",
       "      <td>0.582095</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013506</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.381325</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 5, 'wei...</td>\n",
       "      <td>0.597173</td>\n",
       "      <td>0.576991</td>\n",
       "      <td>0.603540</td>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.592357</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_metric  \\\n",
       "0       0.027741      0.005889         0.398121        0.005112    euclidean   \n",
       "1       0.022611      0.005421         0.374393        0.006517    euclidean   \n",
       "2       0.016816      0.002095         0.388486        0.010165    euclidean   \n",
       "3       0.016576      0.003577         0.369897        0.011103    euclidean   \n",
       "4       0.013506      0.000430         0.381325        0.023680    euclidean   \n",
       "\n",
       "  param_n_neighbors param_weights  \\\n",
       "0                 1       uniform   \n",
       "1                 1      distance   \n",
       "2                 3       uniform   \n",
       "3                 3      distance   \n",
       "4                 5       uniform   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'metric': 'euclidean', 'n_neighbors': 1, 'wei...           0.549470   \n",
       "1  {'metric': 'euclidean', 'n_neighbors': 1, 'wei...           0.549470   \n",
       "2  {'metric': 'euclidean', 'n_neighbors': 3, 'wei...           0.574205   \n",
       "3  {'metric': 'euclidean', 'n_neighbors': 3, 'wei...           0.574205   \n",
       "4  {'metric': 'euclidean', 'n_neighbors': 5, 'wei...           0.597173   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.552212           0.582301           0.568142           0.566372   \n",
       "1           0.552212           0.582301           0.568142           0.566372   \n",
       "2           0.546903           0.607080           0.576991           0.605310   \n",
       "3           0.546903           0.607080           0.576991           0.605310   \n",
       "4           0.576991           0.603540           0.584071           0.600000   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.563694        0.011895               21  \n",
       "1         0.563694        0.011895               21  \n",
       "2         0.582095        0.022312               17  \n",
       "3         0.582095        0.022312               17  \n",
       "4         0.592357        0.010112               15  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame(knn_gridsearch.cv_results_)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the set of hyperparameters that achieved the best score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign the best fit model (`best_estimator_`) to a variable and score it on the test data.**\n",
    "\n",
    "Compare this model to the bechmark accuracy and your default KNN (0.79)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=21, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier(metric='manhattan', n_neighbors=21, weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6443736730360934"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knn = knn_gridsearch.best_estimator_\n",
    "best_knn.score(Z_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.5987261146496815\n",
      "default KNN: 0.613588110403397\n"
     ]
    }
   ],
   "source": [
    "print('baseline:', np.mean(y_test))\n",
    "print('default KNN:', neighbors.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='caution'></a>\n",
    "\n",
    "## A Word of Caution on Grid searching\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn models often have many options/hyperparameters with many different possible values. It may be tempting to search over a wide variety of them. In general, this is not wise.\n",
    "\n",
    "Remember that **gridsearch searches over all possible combinations of hyperparamters in the paramter dictionary!**\n",
    "\n",
    "The KNN model class takes a wider range of options during instantiation than we have explored. Imagine that we had this as our parameter dictionary:\n",
    "\n",
    "```python\n",
    "parameter_grid = {\n",
    "    'n_neighbors':range(1,151),\n",
    "    'weights':['uniform','distance',custom_function],\n",
    "    'algorithm':['ball_tree','kd_tree','brute','auto'],\n",
    "    'leaf_size':range(1,152),\n",
    "    'metric':['minkowski','euclidean'],\n",
    "    'p':[1,2]\n",
    "}\n",
    "```\n",
    "\n",
    "**How many different combinations will need to be tested?\n",
    "\n",
    "| Parameter | Potential Values | Unique Values |\n",
    "| --- | ---| ---: |\n",
    "| **n_neighbors** | int range 1-150 | 150 |\n",
    "| **weights** | strs:  \"uniform\", \"distance\" or user defined function | 3 |\n",
    "| **algorithm** | strs: \"ball_tree\", \"kd_tree\", \"brute\", \"auto\" | 4 |\n",
    "| **leaf_size** | int range 1-151 | 151 |\n",
    "| **metric** | str: \"minkowski\" or 'euclidean' type | 2 |\n",
    "| **p** | int: 1=manhattan_distance, 2= euclidean_distance | 2 |\n",
    "|| <br>_150 \\* 3 \\* 4 \\* 151 \\* 2 \\* 2 = n combinations_ <br><br>| _1,087,200_ |\n",
    "\n",
    "Over a million tests *before we even consider the number of cross-validation folds!*\n",
    "\n",
    "If we're not careful, gridsearching can quickly blow up, taking our time and machine with it. A lot of the hyperparameters we put in the dumb example above are either redundant or not useful.\n",
    "\n",
    "> **It is extremely important to understand what the hyperparameters do and think critically about what ranges are useful and relevant to your model!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to survey a space of possible parameters without testing every single parameter is to use a randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:    5.5s finished\n"
     ]
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':[1,3,5,9,15,21],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "\n",
    "knn_randomsearch = RandomizedSearchCV(KNeighborsClassifier(), knn_params, cv=5, n_iter=10, verbose=1, n_jobs=2, random_state=42)\n",
    "\n",
    "knn_randomsearch = knn_randomsearch.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance', 'n_neighbors': 21, 'metric': 'euclidean'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_randomsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6985138004246284"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(Z_train, y_train)\n",
    "model.score(Z_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    1.6s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10.0, 100],\n",
       "                         'fit_intercept': [True, False], 'max_iter': [1000],\n",
       "                         'solver': ['lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000],\n",
    "    'C': [0.01, 0.1, 1, 10.0, 100],\n",
    "    'fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "grid.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7016985138004246"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_\n",
    "grid.best_estimator_.score(Z_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dataframemapper',\n",
       "                 DataFrameMapper(default=StandardScaler(copy=True,\n",
       "                                                        with_mean=True,\n",
       "                                                        with_std=True),\n",
       "                                 df_out=False, features=[], input_df=False,\n",
       "                                 sparse=False)),\n",
       "                ('gridsearchcv',\n",
       "                 GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                              estimator=LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,...\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False),\n",
       "                              iid='warn', n_jobs=-1,\n",
       "                              param_grid={'C': [0.01, 0.1, 1, 10.0, 100],\n",
       "                                          'fit_intercept': [True, False],\n",
       "                                          'max_iter': [1000],\n",
       "                                          'solver': ['lbfgs']},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "pipe = make_pipeline(mapper, grid)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7016985138004246"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Gridsearch searches model hyperparameters!\n",
    "- Model parameter = coefficient, intercept\n",
    "- Model hyperparameter = n_neighbors, distance_metric, etc\n",
    "\n",
    "Hyperparameters control model behavior.\n",
    "\n",
    "### Gridsearch syntax\n",
    "```python\n",
    "param_grid = {\n",
    "    \"model_hyperparameter_1\": [\"params\", \"to\", \"test\"],\n",
    "    \"model_hyperparameter_2\": [\"params\", \"to\", \"test\"],\n",
    "    \"model_hyperparameter_3\": [\"params\", \"to\", \"test\"],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator, \n",
    "    param_grid, \n",
    "    cv = 5, \n",
    "    verbose = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "```\n",
    "\n",
    "### Gridsearch Evaluation\n",
    "\n",
    "- grid.best_score_\n",
    "- grid.best_params_\n",
    "- grid.best_estimator_\n",
    "- grid.cv_results\n",
    "\n",
    "Be careful when searching hyperparameters!  \n",
    "\n",
    "```python\n",
    "parameter_grid = {\n",
    "    'n_neighbors':range(1,151),\n",
    "    'weights':['uniform','distance',custom_function],\n",
    "    'algorithm':['ball_tree','kd_tree','brute','auto'],\n",
    "    'leaf_size':range(1,152),\n",
    "    'metric':['minkowski','euclidean'],\n",
    "    'p':[1,2]\n",
    "}\n",
    "```\n",
    "#### `150 * 3 * 4 * 151 * 2 * 2 = 1,087,200` combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "## Practice: Grid Search Regularization Penalties with Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression models can also apply the Lasso and Ridge penalties. The `LogisticRegression` class takes these regularization-relevant hyperparameters:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`penalty`** | `'l1'` for Lasso, `'l2'` for Ridge |\n",
    "| **`solver`** | Must be set to `'liblinear'` for the Lasso penalty to work. |\n",
    "| **`C`** | The regularization strength. Equivalent to `1./alpha` |\n",
    "\n",
    "**You should:**\n",
    "1. Fit and validate the accuracy of a default logistic regression on the basketball data.\n",
    "- Perform a gridsearch over different regularization strengths and Lasso and Ridge penalties.\n",
    "- Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy and the default model.\n",
    "- Look at the best parameters found. What was chosen? What does this suggest about our data?\n",
    "- Look at the (non-zero, if Lasso was selected as best) coefficients and associated predictors for your optimized model. What appears to be the most important predictors of winning the game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_params = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'solver':['liblinear'],\n",
    "    'C':np.logspace(-5,0,100)\n",
    "}\n",
    "\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), gs_params, cv=5, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
